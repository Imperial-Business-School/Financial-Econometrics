{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"PB1 Stylised Facts\"\n",
        "author: \"Roald Versteeg\"\n",
        "date: \"2024-02-08\"\n",
        "output:\n",
        "  html_document:\n",
        "    df_print: paged\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "R"
      },
      "source": [
        "knitr::opts_chunk$set(echo = TRUE)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Stylised Facts of financial asset returns\n",
        "## Financial Econometrics - Roald Versteeg\n",
        "\n",
        "In this worksheet, we are trying to replicate some of the most prevalent stylised empirical facts of financial returns.\n",
        "\n",
        "\n",
        "Cont (2001) discusses the following eleven stylised facts of asset returns:\n",
        "\n",
        "1. **Absence of autocorrelations**: with the exception of intraday data.\n",
        "\n",
        "2. **Heavy tails**: excess kurtosis (>3) and returns have a Pareto-like tail. (leptokurtic returns)\n",
        "\n",
        "3. **Gain/loss asymmetry**: negative skew (<0), with more extreme losses than extreme gains.\n",
        "\n",
        "4. **Aggregational Gaussianity**: the distribution of annual returns is closer to a normal than daily returns.\n",
        "\n",
        "5. **Intermittency**: return volatility is not constant over time but displays heteroskedasticity.\n",
        "\n",
        "6. **Volatility clustering**: measures of volatility, like absolute returns, have positive autocorrelation.\n",
        "\n",
        "7. **Conditional heavy tails**: After modelling conditional heteroskedasticity (with eg. GARCH),  conditional returns are closer to normality, but still have some excess kurtosis remaining.\n",
        "\n",
        "8. **Slow decay of autocorrelation in absolute returns**: \n",
        "This is sometimes interpreted as a sign of long-range dependence.\n",
        "\n",
        "9. **Leverage effect**: negative returns have a bigger impact on volatility than positive returns.\n",
        "\n",
        "10.**Volume/volatility correlation**: trading volume is correlated with all measures of volatility.\n",
        "\n",
        "11. **Asymmetry in time scales**: coarse-grained measures of volatility predict fine-scale volatility better than the other way round.\n",
        "\n",
        "Cont, R. (2001), Empirical properties of asset returns: stylized facts and statistical issues, Quantitative Finance, **1** (2), pp. 223 - 236. \\\\\n",
        "http://rama.cont.perso.math.cnrs.fr/pdf/empirical.pdf \n",
        "\n",
        "In this worksheet, we'll check for the presence of 1., 2.,  5., 6., and 8.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "R"
      },
      "source": [
        "library(tidyverse)\n",
        "library(quantmod)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Loading the data\n",
        "\n",
        "Let's begin with loading the data using the Quantmod package, which can load data from sources like Yahoo, Google, or FRED.\n",
        "Alternatively you can load the data directly from a .csv file you have locally.\n",
        "\n",
        "For this excercise I've chosen daily data of the New York Stock Exchange (NYSE), running from January 1985 to January 2024.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "R"
      },
      "source": [
        "ticker <- \"^NYA\" \n",
        "startDate <- \"1985-01-01\" \n",
        "endDate <- \"2024-01-01\" # You can use to=Sys.Date() to get data up to the current date\n",
        "\n",
        "rawData <- getSymbols(ticker, \n",
        "                      auto.assign = FALSE,\n",
        "                      from = startDate, \n",
        "                      to = endDate, \n",
        "                      src=\"yahoo\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "There is a couple of neat things we can do while the series is expressed as an xts (extended time series), which it will be when you download it from QUantmod like easily converting the frequency of the data or calculating the sum per month.I've provided code below that shows how to calculate log returns and convert daily returns into monthly and yearly returns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "R"
      },
      "source": [
        "ret.daily <- log( rawData$NYA.Close) - stats::lag( log( rawData$NYA.Close) )\n",
        "\n",
        "#log returns are summable; calculate as the sum of the daily returns\n",
        "ret.monthly <- apply.monthly(ret.daily, sum)\n",
        "ret.yearly <- apply.yearly(ret.daily, sum)\n",
        "\n",
        "# notice the difference in the date notation between the two.\n",
        "price.monthly <- to.monthly(rawData)\n",
        "price.monthly2 <- to.period(rawData, period = \"months\")\n",
        "price.yearly <- to.yearly(rawData)\n",
        "\n",
        "# calculate the monthly returns as the difference in the monthly prices\n",
        "ret.monthly2 <- log( price.monthly$rawData.Close) - stats::lag( log( price.monthly$rawData.Close) )\n",
        "ret.yearly2 <- log( price.monthly$rawData.Close) - stats::lag( log( price.monthly$rawData.Close) )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Now I'm going to ignore the code chunk above, and instead convert the data to a dataframe.\n",
        "We'll have to calculate log returns again and scale as appropriate to monthly or yearly. We'll drop the other series, as we don't need them going forward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "R"
      },
      "source": [
        "data <- data.frame(date=index(rawData), coredata(rawData)) \n",
        "\n",
        "data <- data %>%\n",
        "  mutate( Price = log(NYA.Close) ) %>% #rename for convenience\n",
        "  mutate( Ret = Price - lag(Price) ) %>% #log returns\n",
        "  select( date, Price, Ret) %>%\n",
        "  drop_na() #drop missing observations (mainly the first observation)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's plot the data. We can see that the DJI itself is exhibiting the characteristic random walk behaviour, whilst the returns are mean reverting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "R"
      },
      "source": [
        "ggplot(data=data, aes(x=date, y=Price)) +\n",
        "  geom_line() + \n",
        "  ylab(\"Log Prices\") +\n",
        "  xlab(\"Date\") +\n",
        "  ggtitle(\"NYSE Log Price\")\n",
        "\n",
        "ggplot(data=data, aes(x=date, y=Ret)) +\n",
        "  geom_line() + \n",
        "  ylab(\"Returns\") +\n",
        "  xlab(\"Date\") +\n",
        "  ggtitle(\"NYSE Log Returns\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Autocorrelations\n",
        "\n",
        "Let's check for autocorrelations, using the acf function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "R"
      },
      "source": [
        "acf_p<-acf( data$Price, lag.max=10 )\n",
        "\n",
        "acf_r<-acf( data$Ret, lag.max=10,  \n",
        "            ylim=c( -0.1 , 0.1) #I'm limiting the Y axis to make the values more visible\n",
        "            ) \n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "We can see that for the price data, the ACF function shows that the autocorrelation stays at 1.0 even after ten periods. We will ignore this for now, but this will prove important later.\n",
        "\n",
        "For the returns, the first autocorrelation seems to be significantly negative (the blue lines indicate the confidence interval).\n",
        "\n",
        "## Assymetry: Skewness\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "R"
      },
      "source": [
        "library(moments)\n",
        "skewness(data$Ret)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "We can see that the daily returns indeed exhibit negative skew. We can use the hist() and density() functions to plot the empirical distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "R"
      },
      "source": [
        "hist(data$Ret, breaks=100, main=\"Daily Returns Histogram\")\n",
        "plot(density(data$Ret), main=\"Daily Returns Density\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Heavy Tails: Kurtosis\n",
        "\n",
        "To check for heavy tails, we can check the kurtosis. For a normal distribution, the kurtosis is 3. If the number is higher, we are dealing with excess kurtosis, also known as heavy or fat tails.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "R"
      },
      "source": [
        "kurtosis(data$Ret) # Note: this is not excess kurtosis\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively we can plot the distribution on a Quantile-Quantile plot (Q-Q plot), which compares the actual distribution (circles) against a normal distribution with a mean an standard deviation equal to the sample estimates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "R"
      },
      "source": [
        "qqnorm(data$Ret)\n",
        "qqline(data$Ret)\n",
        "\n",
        "# The car library contains another functions that can also plot the QQ against a normal.\n",
        "# library(car)\n",
        "# qqPlot(data$Ret)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The graph shows that the empirical distribution of returns has significantly more outliers both on the left and the right, visualising the fat tails.\n",
        "\n",
        "### Testing for Normality\n",
        "\n",
        "The Jarque-Bera test checks whether the sample skew and kurtosis are jointly equal to those of a normal distribution (A normal has skew = 0 and Kurtosis = 3).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "R"
      },
      "source": [
        "jarque.test(data$Ret)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The JB test shows that the distribution shows significant deviations away from normality.\n",
        "\n",
        "## Volatility Clustering\n",
        "\n",
        "A common proxy for volatility is the squared return. \n",
        "Absolute returns can be thought of as a proxy of the standard deviation. Let's plot the absolute returns and the squared returns and their autocorrelation to see if there is any evidence for volatility clustering.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "R"
      },
      "source": [
        "data <- data %>% \n",
        "  mutate( Ret_sq = Ret^2 ) %>% \n",
        "  mutate( Ret_abs = abs(Ret) )\n",
        "\n",
        "ggplot(data=data, aes(x=date, y=Ret_abs)) +\n",
        "  geom_line() + \n",
        "  ylab(\"Absolute Returns\") +\n",
        "  xlab(\"Date\") +\n",
        "  ggtitle(\"DJI Absolute Returns\")\n",
        "\n",
        "acf_r_abs <- acf( data$Ret_abs, lag.max = 10)\n",
        "\n",
        "ggplot(data=data, aes(x=date, y=Ret_sq)) +\n",
        "  geom_line() + \n",
        "  ylab(\"Squared Returns\") +\n",
        "  xlab(\"Date\") +\n",
        "  ggtitle(\"DJI Squared Returns\")\n",
        "\n",
        "acf_r_sq <- acf( data$Ret_sq, lag.max = 10)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Looking at the line graph, we can see that there are periods where the absolute returns are consistently spiking, which indicate periods of high volatility, and periods where the absolute returns are consistently low, indicating periods of low volatility. This seems to confirm the stylized fact of Intermittance.\n",
        "\n",
        "When we plot the ACF of absolute returns, we can see that the autocorrelation coefficient stays fairly stable between 0.2 and 0.4, even after 10 periods. Again, this indicates that there is a temporal dependency in volatility, that does not seem to die out even at fairly long horizons.\n",
        "\n",
        "## Bonus: Extreme Value Theory and Tail Behaviour\n",
        "One way to capture the tail dynamics is using Extreme Value Theory (EVT). The Hill (1975) estimator is a common tail estimator which captures the speed of the tail decay. The tail parameter can also be interpreted as the number of bounded moments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "R"
      },
      "source": [
        "#install.packages(evir)\n",
        "library(evir)\n",
        "hillplot <- evir::hill(data$Ret, start = 0.04*length(data$Ret), end = 0.05*length(data$Ret), option = \"alpha\")\n",
        "\n",
        "print(last(hillplot$y))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The tail estimate depends a lot on the exact choice where the tail starts. Here we consider between 4% and 5% of the sample. The estimates show that the tail exponent is roughly 2.8, which is very low. The tail exponent can also be interpreted as the number of moments of a distribution that is bounded. In this case, it implies that the 2nd moment (variance) is bounded, but the third moment (skew) and fourth moment (kurtosis) are quite likely not very well defined. Again reinforcing how much stock returns deviate away from normality.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}